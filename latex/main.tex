\documentclass{article}
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

\usepackage[preprint]{jmlr2e}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\binoppenalty=\maxdimen
\relpenalty=\maxdimen

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Short headings should be running head and authors last names

\ShortHeadings{Learning with Mixtures of Trees}{Côme \& Thouvenin}
\firstpageno{1}

\begin{document}

\title{Learning with Mixtures of Trees}

\author{\name Aymeric CÔME \email aymeric.come.etu@univ-lille.fr \\
       \addr Master Data Science\\
       University of Lille, Centrale Lille, IMT Lille-Douai\\
       Villeneuve d'Ascq, France
       \AND
       \name Pierre-Antoine THOUVENIN \email pierre-antoine.thouvenin@centralelille.fr \\
       \addr Assistant professor and member of SigMA team\\
       Université de Lille, CNRS, Centrale Lille, CRIStAL\\
       Villeneuve d'Ascq, France}

\maketitle

\begin{abstract}%
  Hyperspectral unmixing is the task of infering the abundance of pure materials in an image from the observed spectra. The resulting spectrum indeed depends on some characteristical features of the End-Members (EMs), however this is a challenging problem because not only it is nonlinear, but also miscellaneous factors can change the output, which is known as Spectral Variability (SV). To tackle this problem, a paper \citep{janiczek_differentiable_2020} introduced a physics-based, differentiable model to realistically capture EM HV, and then used it in an optimisation framework in order to find promising results for spectral unmixing. However, while the dispersion model seems fruitful, the optimisation part raises questions. In the following is presented how we try to improve the unmixing process.
\end{abstract}

\begin{keywords}
Hyperspectral Unmixing, Spectral Unmixing, Alternating Minimization, Proximal
\end{keywords}

\section{Introduction}
\textbf{Context:} what is spectral unmixing, some applications (remote sensing)\\
\textbf{Variability models:} quick description of explicit (additive/multiplicative), black-box (Deep) and physics models.\\
\textbf{Contribution:} take advantage of the dispersion model, with more prior information (regularisation) than in Janiczek paper to obtain a more relevant model, while keeping a differentiable problem for an easier resolve (cf natural model is ugly).\\

\textbf{First draft:} In many real-world applications like space exploration, medicine, environment monitoring, mineral detection, hyperspectral images (HI) (consisting of samples of electromagnetic spectra at various wavelengths) can put into evidence some key characteristics of the location monitored. In particular, it is commonly supposed that each present EM have its own signature spectrum, which contributes to the HI depending on its abundance in each pixel. Hence, being able to unmix these characteristic signatures from HI leads to a precise detection of materials in a scene.

However, despite the high spectral resolution of hyperspectral cameras, there are technical limits that must be taken into account; in particular low spatial resolution is to be expected, which leads to a mixture of EMs in each piel. While many unmixing algorithms already have been proposed, the physics behind is often too complex to be fully modelled.

A first difficulty to handle is the SV: depending on the scene observed, an EM might have a different signature than what we were expecting from previous observations. This variability can even occurs locally, between pixels of a same HI, which actually question the hypothesis of one signature corresponding to one EM, rather than a class of signatures. Multiple factors can contribute to this variability, and we distinguish two kinds: intrisic and external. Intrisic variability is due to the natural differences of the EM samples across the HI (e.g. grass is greener on the other side because the neighbor water it) while external variability refers to the environment influence, like light and shade or the quality of the hyperspectral camera used.

In the end, the unmixing problem is a very complex one, and requires advanced technics to overcome the difficulties. We will present some of them in the following section.

\section{Variability models}
Review of the literature.

\subsection{Explicit models}
Learn variability from an a priori structure. Borsoi 2020

\subsection{Deep models}
Learn variability model directly from data, using deep structures. Borsoi 2019 (generative), Hong 2021, Ozkan 2019 (convolution)

\subsection{Physics-based models}\label{sec:dispersion}
Rely on physics properties to infer (complex) models of variability. Try to keep it differentiable. Janiczek 2020.


\section{Problem statement}
Based on the elements and observations we have seen so far, we come up with a formulation for the problems we are willing to tackle.

\subsection{Reconstruction model}
The first part of a spectral unmixing strategy is to try to reconstruct the target HI from a specified model.

Our approach consists in making use of the dispersion model presented in Section \ref{sec:dispersion}, within what we hope to be a better framework. So, the reconstructed spectra are generated by the dispersion model from the physics parameters. More precisely, we suppose that each EM is present in a state characterised by those parameter values in every pixel; and then each EM is associated to a signature spectrum derived from the dispersion model. The SV is modeled here by the variation of the parameter values through the HI, which in turn gives different signature spectra over the pixels. Regarding the unmixing, we decided to use Linear Unmixing once again, so that our model would still compare to the one in the original paper. We then introduce abundances for each EM, accross the HI.

\subsubsection{Setting up the problem}

We consider a HI $B \in \mathbb{R}^{N \times M \times S}$ consisting of $N \times M = \Pi$ pixels, each pixel giving a spectrum evalutated at wavelenghts $w_1,\dots, w_S$. We assume that there are $P$ EMs in the HI, characterised by sets of vectors of parameters $\theta_1,\dots, \theta_P$. As we put ourselves in the dispersion model framework, each $\theta_p \in \mathbb{R}^{N \times M \times K \times 4}$ actually corresponds to the parameters $\rho, \omega_0, \gamma, \epsilon_r$ introduced in the previous section, with different values for different pixels (for sake of lisibility, we define $T = K \times 4$ the number of parameters per EM). Then, we define the matrix of abundances $A \in [0, 1]^{N \times M\times P}$ such that $a_{i, j, p}$ corresponds to the abundance of EM $p$ in pixel $(i, j)$, and $\|A_{i, j, 1:P}\|_1 = 1$. Finally, $E(\theta_1,\dots, \theta_P) = E(\Theta) \in \mathbb{R}^{N \times M \times P \times S}$ gives the reference spectrum of each EM at each pixel, derived from the dispersion model: $E(\Theta)_{i, j, p, s} = \epsilon(\theta_{i, j, p}; w_s)$.

\subsubsection{Model of the hyperspectral image}

In accordance to a commonly used approach in the literature ADD CITATION, we model the HI as follows:
$$B = E(\Theta) \cdot A + \eta$$
where $\eta$ is a white noise and $E(\Theta) \cdot A = \left( \sum_{p=1}^P E(\Theta)_{i,j,p,s} a_{i,j,p} \right)_{i,j,s}$ is the sum of the signature spectra weighted by abundances, for each pixel and spectral channel.

So far we just have reformulated the problem from \cite{janiczek_differentiable_2020}, however we wish to add prior information about the elements of the problem, as discussed in the following sections. This will be expressed by additional terms in the objective function. In the end, the optimisation problem states as:

\begin{equation}
  \label{eq:objective}
  \argmin_{\Theta \in \Delta,\ A \in \Upsilon} H(\Theta, A) + f(\Theta) + g(A)
\end{equation}
with $H(\Theta, A) = \frac{1}{2} \|B - E(\Theta) \cdot A\|_{F, 2}^2$, and $Upsilon$ and $\Delta$ are constraint spaces presented afterwards. The norm defined by $\|Q\|_{F, 2} = \sqrt{\sum_{1 \leq i \leq N,\\1 \leq j \leq M} \|Q_{i, j, 1:S}\|_2^2}$ is the Froebenius norm applied on the square norm over spectrum for every pixel. $f(\Theta)$ and $g(A) = g_c(A) + g_s(A)$ are regularisation terms, enforcing priors on the parameters.

This formulation corresponds to finding the maximum likelihood in presence of white noise, with respect to priors.

\subsection{Priors on the endmembers}
In this section are explained the priors introduced regarding the EMs present in the HI.

\subsubsection{Physics-based constraints}

The main prior regarding EMs is that they follow the physics-based dispersion model, controlled by the parameters $\Theta$. These parameters actually represent physical quantities (frictional force, band strength...): in that regard, and alike what was done in \cite{janiczek_differentiable_2020}, their values should stay in a plausible range, determined by a physical analysis. From now on, we will design by $\Delta$ the space in which the parameters $\Theta$ live:
$$\Delta = [\rho_{min}, \rho_{max}] \times [\omega_{0, min}, \omega_{0, max}] \times [\gamma_{min}, \gamma_{max}] \times [\epsilon_{r, min}, \epsilon_{r, max}]$$

\subsubsection{Smoothness}\label{sec:EM-smoothness}

As advertised earlier, our approach to model the SV is to let the physical parameters $\Theta$ of the EMs vary across the pixels of $B$. Yet, to keep a spatial consistency we want to ensure a smooth variation, to represent the idea that, as the environmental conditions are roughly continuous with respect to space dimensions, so are material states and signature spectra. For instance, the quantity of water received by a lawn is close to the one received by the adjacent grass. Obviously, this hypothesis is not always appropriate, as your neighbor might water his lawn way more than you; yet it should still be a relevant hypothesis in most real-world applications ADD CITATION.

To do so, we consider the \emph{discrete gradient} operator, as in CITE(CONDAT2014). It consists in the horizontal and vertical differences between neighbour pixels, so that for an $N \times M$ image $Y$ we have:

\begin{align*}\label{eq:discrete-grad}
  D(Y) &= (u_h (Y),\ u_v (Y)),\\
  u_h (Y) [i, j] &= Y_{i+1, j} - Y_{i, j} \text{ if }i + 1 \leq N,\ 0 \text{ otherwise}\\
  u_v (Y) [i, j] &= Y_{i, j+1} - Y_{i, j} \text{ if }j + 1 \leq M,\ 0 \text{ otherwise}
\end{align*}

This operator is a measure of the absolute spatial variation of the pixel values, so in order to promote smoothness we want to keep these differences small. We quantify it with:

\begin{align*}
  \| D(Y) \|_{1,2}^2 = \sum_{i=1}^N \sum_{j=1}^M \sqrt{ u_h (Y_{x})[i, j]^2 + u_v (Y_x)[i, j]^2 }
\end{align*}

Finally, we enforce smoothness by minimizing the sum of the squared variations, so that the corresponding term is (for some penalty factor $\gamma >0$):

\begin{align}\label{eq:f}
  f(\Theta) &= \gamma \mathcal{L}(\Theta)\\
  &= \gamma \sum_{t = 1}^{T} \| D(\Theta_{:,:,t}) \|_{1,2}^2
\end{align}

Note that one might prefer to ensure smoothness of the spectra rather than the parameters; we would then have:
\begin{align*}
  f(\Theta) &= \gamma \mathcal{L}(E(\Theta))
\end{align*}

\subsection{Priors on the abundances}
In our physics-inspired model, we introduced the material abundances of each EM in every pixel, and, in accordance with the Linear Unmixing Model, we assume that an EM contributes to the mixture signal proportionally to its abundance. To remain in a relevant case, we need to ensure that some properties are respected for the abundances.

\subsubsection{Constraints}

Based on their physical meaning, abundances of EMs must obviously be positive and sum to 1 in each pixel. These constraints are usually formulated using an indicator operator:

$$\iota_\Upsilon (A) = \begin{cases} 0 \text{ if A}\in \Upsilon\\ +\infty \text{ otherwise} \end{cases}$$
where
$$\Upsilon = \{Q \in \mathbb{R}^{\Pi \times P}_+\ |\ \| Q_{i, j, 1:P} \|_1 = 1 \ \forall 1 \leq i \leq N, 1 \leq j \leq M\}$$
is the simplex where abundances live.

However, this constraint introduces a significant discontinuity in the problem, then losing differentiability. To circumvent this issue, we have chosen to relax the constraint term into a smooth penalty term, so that we strongly encourage abundances to stay close the $\Upsilon$ while being differentiable. This relaxation should be acceptable as our model is linear with respect to the abundances, so that the mixture spectrum obtained with an abundance map close to $\Upsilon$ should still provide a good reconstruction.

The idea is to strongly promote values close to $\Upsilon$ for $A$, thanks to an approximation of the indicator. Such approach has already been presented and used in the literature, in the form of \emph{log-barrier method} ADD CITATION. In particular, we chose the \emph{log-barrier extension} introduced in CITE(Kervadec2020). This operator is convex, continuous and twice-differentiable, and is defined as follows, for some hyper-parameter $t > 0$:
$$\forall z\in \mathbb{R},\ \tilde\Psi_t (z) = \begin{cases} - \frac{1}{t} \log (-z) & \text{if } z \leq -\frac{1}{t^2}\\ tz - \frac{1}{t} \log(\frac{1}{t^2}) + \frac{1}{t^2} & \text{otherwise} \end{cases}$$

The higher $t$, the better the quality of the approximation; however one should be cautious not to overwhelm the other terms in the problems - this is discussed the next section.

To translate the sum-to-one constraint on the abundances, we rewrite it as a sum-greater-than-one and sum-lesser-than-one constraint, in addition to the positivity constraint. Finally, we come up with the following penalty term, $\zeta > 0$ a penalty factor:
\begin{equation}\label{eq:g_c}
  g_c (A) = \zeta \cdot \Bigg[ \sum_{i = 1}^N \sum_{j = 1}^M \sum_{p=1}^P \tilde\Psi_\nu (-a_{i,j,p}) + \sum_{i = 1}^N \sum_{j = 1}^M \left[ \tilde\Psi_\nu(\sum_{p=1}^P a_{i,j,p} - 1) + \tilde\Psi_\nu(1 - \sum_{p=1}^Pa_{i,j,p}) \right] \Bigg]
\end{equation}

\subsubsection{Sparsity}

Regarding the abundances, we whish to enforce sparsity of abundances in every pixel; we would rather have a few EMs in significant abundances than a lot of EMs in small quantities. This prior is often relevant in real-wolrd applications ADD CITATION.

Common choices to do so are the $\ell_0$ count measure, the $\ell_1$ norm, or the $\ell_p \ (p<1)$ quasi-norm ADD CITATION. However, these terms are hard to handle during optimisation, as they introduce non-convexity and discontinuity. Hence, to keep once again our problem differentiable, we ressort to the SPOQ regularisation, introduced in CITE(Cherni2020). It gives a smooth approximation of $\ell_0$.

For some hyper-parameters $p\in \, ]0, 2[$, $q\in \, [2, +\infty[$, and $(\alpha, \eta) \in \, ]0, +\infty [^2$, the SPOQ regularisation term is defined as follow, for any vector $x \in \mathbb{R}^N$:
$$SPOQ(x) = \log \left( \frac{(\ell_{p,\alpha}^p (x) + \beta^p)^{1/p}}{\ell_{q,\eta} (x)} \right)$$
        where
        $$\ell_{p,\alpha} (x) = \left( \sum_{n=1}^N (x_n^2 + \alpha^2)^{p/2} - \alpha^p \right)^p$$
        $$\ell_{q,\eta} (x) = \left( \eta^q + \sum_{n=1}^N |x_n|^q \right)^{1/q}$$

The choice of the hyper-parameters is discussed in the next section.

We then apply the SPOQ pseudo-norm on the vector of abundances in every pixel of the HI:
$$SPOQ(A) = (SPOQ(a_{i,j,1:P}))_{i, j} \in \mathbb{R}_+^{N \times M}$$

In the end, we define the sparsity term on abundances as (with $\delta > 0$ a penalty term):
\begin{equation}\label{eq:g_s}
  g_s(A) = \frac{\delta}{N \times M} \| SPOQ(A) \|_{1}
\end{equation}

It simply is a sum of the SPOQ norms over the pixels.

\subsubsection{Smoothness}
Finally, one could also require the abundances to vary smoothly across space. Hence, in a similar way to Section \ref{sec:EM-smoothness}, we could add a $\beta \mathcal{L}(A)$ term to the regularisation on abundances. We will not consider it here though, for simplicity and because we value the sparsity prior more.


\section{Inference algorithm}

The optimisation problem presented in (\ref{eq:objective}) is composed of one differentiable, non-convex block $H(\Theta, A) + f(\Theta)$ and one non-convex, non-continuous block $\delta\|A\|_{1,0} + \iota_\Upsilon (A)$, and deal with two vectors of parameters $\Theta$ and $A$.

The first alternative to non-differentiability is to use Moreau's proximal operator: a first-order approximation of a gradient descent step. We would then operate an alternating minimization on $\Theta$ (differentiable) and $A$ (non-differentiable). However, algorithms involving this operator typically rely on the knowledge of a (tight) Lipschitz constant of the differentiable part to derive a fitting learning rate, and due to the complex definition of the dispersion model we do not have access to such a constant. Moreover, one can't easily derive the close form of the proximal operator of an addition of two non-differentiable, non-convex terms.

Related families of optimisation algorithms would be subgradient or bundle algorithms, unfortunately they do not seem to apply to our problem, mainly because of the non-convexity and the fact that in the constraint $\iota_\Upsilon$ on $A$, $\Upsilon$ is a simplex rather than a convex space.

Hence, instead of testing such approaches without guarantees, we chose a Lagrangian-based method from Kervadec2020, using a \emph{log-barrier extension} to replace the constraint term for $A$. Then, thanks to Cherni2020 we approximate the sparsity term on $A$ with a smooth approximation, so that as written in (\ref{eq:general}) we put ourselves in a smooth context.

Finally, thanks to these rewritings we are able to do a Stochastic Gradient Descent-like optimisation with Adam-which has already shown to be an efficient approach for such problems. A discussion can be made on other Adam-like algorithms in a non-convex problem (cf Chen2019), yet we will stick to the classical Adam for simplicity reasons. In particular, we shall use the Autodiff feature from PyTorch rather than deriving by hand the awfully complex gradient of (\ref{eq:general}).

To wrap-up, the inclusion of our priors raised some major optimisation issues, due to which we have been forced to resort to a sub-optimal approach. Hopefully, the behavior will still be good, thanks to the senseful priors and a good initialization provided by the dispersion model. On the other hand, Janiczek \emph{analysis-by-synthesis} consists in an alternating minimization of the reconstruction error, on the abundances and the physics parameters. The main difference with our approach is that the parameters (and then the signature spectrum) of EMs are assumed to be the same all accross the HI.

\section{Experiments}
\subsection{Experimental settings}
\textbf{Datasets:} Describe datasets used for evaluation of the algorithm. Compare with the ones from Janiczek paper.

\textbf{Evaluation criteria:} RMSE, ASAM, MSE for abundances (per EM and for all), reconstruction error $H(\Theta, A)$. Compare with Janiczek.

\textbf{Implementation and settings:} how the code has been implemented, github link, machine on which it ran. Hyperparameters: learning rate, constraints bounds...

\subsection{Results and Discussion}
Present result tables, images (ground truth/Janiczek/us), signatures (mean, max and min against ground truth and Janiczek).


\section{Conclusion}
\textbf{Future work:} use something better than Linear Unmixing.

%\section*{Acknowledgment}

\acks{Put acknowledgements here}

%\section*{References}

% \bibliographystyle{IEEEtran}
\bibliography{unmixing}


\section{Appendix}

\end{document}
