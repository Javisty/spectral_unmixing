
@article{lucas_using_2018,
	title = {Using {Deep} {Neural} {Networks} for {Inverse} {Problems} in {Imaging}: {Beyond} {Analytical} {Methods}},
	volume = {35},
	issn = {1558-0792},
	shorttitle = {Using {Deep} {Neural} {Networks} for {Inverse} {Problems} in {Imaging}},
	doi = {10.1109/MSP.2017.2760358},
	abstract = {Traditionally, analytical methods have been used to solve imaging problems such as image restoration, inpainting, and superresolution (SR). In recent years, the fields of machine and deep learning have gained a lot of momentum in solving such imaging problems, often surpassing the performance provided by analytical approaches. Unlike analytical methods for which the problem is explicitly defined and domain-knowledge carefully engineered into the solution, deep neural networks (DNNs) do not benefit from such prior knowledge and instead make use of large data sets to learn the unknown solution to the inverse problem. In this article, we review deep-learning techniques for solving such inverse problems in imaging. More specifically, we review the popular neural network architectures used for imaging tasks, offering some insight as to how these deep-learning tools can solve the inverse problem. Furthermore, we address some fundamental questions, such as how deeplearning and analytical methods can be combined to provide better solutions to the inverse problem in addition to providing a discussion on the current limitations and future directions of the use of deep learning for solving inverse problem in imaging.},
	number = {1},
	journal = {IEEE Signal Processing Magazine},
	author = {Lucas, Alice and Iliadis, Michael and Molina, Rafael and Katsaggelos, Aggelos K.},
	month = jan,
	year = {2018},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Analytical models, Biological neural networks, deep learning techniques, deep neural networks, DNN, image inpainting, Image reconstruction, image resolution, image restoration, image superresolution, imaging problems, inverse problems, Inverse problems, learning (artificial intelligence), Machine learning, neural nets, Neural networks, Visual systems},
	pages = {20--36},
	file = {IEEE Xplore Abstract Record:/home/javisty/Zotero/storage/3QL625F4/8253590.html:text/html},
}

@article{kervazo_provably_2020,
	title = {Provably robust blind source separation of linear-quadratic near-separable mixtures},
	url = {http://arxiv.org/abs/2011.11966},
	abstract = {In this work, we consider the problem of blind source separation (BSS) by departing from the usual linear model and focusing on the linear-quadratic (LQ) model. We propose two provably robust and computationally tractable algorithms to tackle this problem under separability assumptions which require the sources to appear as samples in the data set. The ﬁrst algorithm generalizes the successive nonnegative projection algorithm (SNPA), designed for linear BSS, and is referred to as SNPALQ. By explicitly modeling the product terms inherent to the LQ model along the iterations of the SNPA scheme, the nonlinear contributions of the mixing are mitigated, thus improving the separation quality. SNPALQ is shown to be able to recover the ground truth factors that generated the data, even in the presence of noise. The second algorithm is a brute-force (BF) algorithm, which is used as a post-processing step for SNPALQ. It enables to discard the spurious (mixed) samples extracted by SNPALQ, thus broadening its applicability. The BF is in turn shown to be robust to noise under easier-to-check and milder conditions than SNPALQ. We show that SNPALQ with and without the BF postprocessing is relevant in realistic numerical experiments.},
	language = {en},
	urldate = {2021-10-21},
	journal = {arXiv:2011.11966 [cs, eess, math, stat]},
	author = {Kervazo, Christophe and Gillis, Nicolas and Dobigeon, Nicolas},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.11966},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Electrical Engineering and Systems Science - Signal Processing, Mathematics - Numerical Analysis, Statistics - Machine Learning},
	annote = {Comment: 23 pages + 24 pages of Appendix containing the proofs},
	file = {Kervazo et al_2020_Provably robust blind source separation of linear-quadratic near-separable.pdf:/home/javisty/Zotero/storage/KQPBQ3Z9/Kervazo et al_2020_Provably robust blind source separation of linear-quadratic near-separable.pdf:application/pdf},
}

@article{bertocchi_deep_2020,
	title = {Deep {Unfolding} of a {Proximal} {Interior} {Point} {Method} for {Image} {Restoration}},
	url = {http://arxiv.org/abs/1812.04276},
	abstract = {Variational methods are widely applied to ill-posed inverse problems for they have the ability to embed prior knowledge about the solution. However, the level of performance of these methods signiﬁcantly depends on a set of parameters, which can be estimated through computationally expensive and timeconsuming methods. In contrast, deep learning oﬀers very generic and eﬃcient architectures, at the expense of explainability, since it is often used as a black-box, without any ﬁne control over its output. Deep unfolding provides a convenient approach to combine variational-based and deep learning approaches. Starting from a variational formulation for image restoration, we develop iRestNet, a neural network architecture obtained by unfolding a proximal interior point algorithm. Hard constraints, encoding desirable properties for the restored image, are incorporated into the network thanks to a logarithmic barrier, while the barrier parameter, the stepsize, and the penalization weight are learned by the network. We derive explicit expressions for the gradient of the proximity operator for various choices of constraints, which allows training iRestNet with gradient descent and backpropagation. In addition, we provide theoretical results regarding the stability of the network for a common inverse problem example. Numerical experiments on image deblurring problems show that the proposed approach compares favorably with both state-of-the-art variational and machine learning methods in terms of image quality.},
	language = {en},
	urldate = {2021-07-15},
	journal = {arXiv:1812.04276 [cs, math]},
	author = {Bertocchi, Carla and Chouzenoux, Emilie and Corbineau, Marie-Caroline and Pesquet, Jean-Christophe and Prato, Marco},
	month = jan,
	year = {2020},
	note = {arXiv: 1812.04276},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	file = {Bertocchi et al_2020_Deep Unfolding of a Proximal Interior Point Method for Image Restoration.pdf:/home/javisty/Zotero/storage/93JGUK5B/2005.06001.pdf:application/pdf},
}

@inproceedings{terris_building_2020,
	title = {Building {Firmly} {Nonexpansive} {Convolutional} {Neural} {Networks}},
	doi = {10.1109/ICASSP40776.2020.9054731},
	abstract = {Building nonexpansive Convolutional Neural Networks (CNNs) is a challenging problem that has recently gained a lot of attention from the image processing community. In particular, it appears to be the key to obtain convergent Plugand-Play algorithms. This problem, which relies on an accurate control of the the Lipschitz constant of the convolutional layers, has also been investigated for Generative Adversarial Networks to improve robustness to adversarial perturbations. However, to the best of our knowledge, no efficient method has been developed yet to build nonexpansive CNNs. In this paper, we develop an optimization algorithm that can be incorporated in the training of a network to ensure the nonexpansiveness of its convolutional layers. This is shown to allow us to build firmly nonexpansive CNNs. We apply the proposed approach to train a CNN for an image denoising task and show its effectiveness through simulations.},
	booktitle = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Terris, Matthieu and Repetti, Audrey and Pesquet, Jean-Christophe and Wiaux, Yves},
	month = may,
	year = {2020},
	note = {ISSN: 2379-190X},
	keywords = {image restoration, Neural networks, Buildings, Convolution, Convolutional neural networks, Image denoising, monotone operators, nonexpansive operator, optimization, Optimization, Signal processing algorithms, Training},
	pages = {8658--8662},
	file = {Terris et al_2020_Building Firmly Nonexpansive Convolutional Neural Networks.pdf:/home/javisty/Zotero/storage/NL5EQFJV/Terris et al_2020_Building Firmly Nonexpansive Convolutional Neural Networks.pdf:application/pdf;IEEE Xplore Abstract Record:/home/javisty/Zotero/storage/N286I5KH/9054731.html:text/html},
}

@article{pesquet_learning_2021,
	title = {Learning {Maximally} {Monotone} {Operators} for {Image} {Recovery}},
	url = {http://arxiv.org/abs/2012.13247},
	abstract = {We introduce a new paradigm for solving regularized variational problems. These are typically formulated to address ill-posed inverse problems encountered in signal and image processing. The objective function is traditionally deﬁned by adding a regularization function to a data ﬁt term, which is subsequently minimized by using iterative optimization algorithms. Recently, several works have proposed to replace the operator related to the regularization by a more sophisticated denoiser. These approaches, known as plug-and-play (PnP) methods, have shown excellent performance. Although it has been noticed that, under some Lipschitz properties on the denoisers, the convergence of the resulting algorithm is guaranteed, little is known about characterizing the asymptotically delivered solution. In the current article, we propose to address this limitation. More speciﬁcally, instead of employing a functional regularization, we perform an operator regularization, where a maximally monotone operator (MMO) is learned in a supervised manner. This formulation is ﬂexible as it allows the solution to be characterized through a broad range of variational inequalities, and it includes convex regularizations as special cases. From an algorithmic standpoint, the proposed approach consists in replacing the resolvent of the MMO by a neural network (NN). We present a universal approximation theorem proving that nonexpansive NNs are suitable models for the resolvent of a wide class of MMOs. The proposed approach thus provides a sound theoretical framework for analyzing the asymptotic behavior of ﬁrst-order PnP algorithms. In addition, we propose a numerical strategy to train NNs corresponding to resolvents of MMOs. We apply our approach to image restoration problems and demonstrate its validity in terms of both convergence and quality.},
	language = {en},
	urldate = {2021-07-01},
	journal = {arXiv:2012.13247 [eess, math]},
	author = {Pesquet, Jean-Christophe and Repetti, Audrey and Terris, Matthieu and Wiaux, Yves},
	month = apr,
	year = {2021},
	note = {tex.ids= Pesquet2020
arXiv: 2012.13247},
	keywords = {Electrical Engineering and Systems Science - Image and Video Processing, Mathematics - Optimization and Control, 47H05, 90C25, 90C59, 65K10, 49M27, 68T07, 68U10, 94A08},
	file = {Pesquet et al. - 2020 - Learning Maximally Monotone Operators for Image Re.pdf:/home/javisty/Zotero/storage/68HV57I6/Pesquet et al. - 2020 - Learning Maximally Monotone Operators for Image Re.pdf:application/pdf;Pesquet et al. - 2021 - Learning Maximally Monotone Operators for Image Re.pdf:/home/javisty/Zotero/storage/6P2ISJC3/Pesquet et al. - 2021 - Learning Maximally Monotone Operators for Image Re.pdf:application/pdf},
}

@article{ongie_deep_2020,
	title = {Deep {Learning} {Techniques} for {Inverse} {Problems} in {Imaging}},
	url = {http://arxiv.org/abs/2005.06001},
	abstract = {Recent work in machine learning shows that deep neural networks can be used to solve a wide variety of inverse problems arising in computational imaging. We explore the central prevailing themes of this emerging area and present a taxonomy that can be used to categorize different problems and reconstruction methods. Our taxonomy is organized along two central axes: (1) whether or not a forward model is known and to what extent it is used in training and testing, and (2) whether or not the learning is supervised or unsupervised, i.e., whether or not the training relies on access to matched ground truth image and measurement pairs. We also discuss the tradeoffs associated with these different reconstruction approaches, caveats and common failure modes, plus open problems and avenues for future work.},
	language = {en},
	urldate = {2020-05-21},
	journal = {arXiv:2005.06001 [cs, eess, stat]},
	author = {Ongie, Gregory and Jalal, Ajil and Metzler, Christopher A. and Baraniuk, Richard G. and Dimakis, Alexandros G. and Willett, Rebecca},
	month = may,
	year = {2020},
	note = {arXiv: 2005.06001},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
	file = {Ongie et al_2020_Deep Learning Techniques for Inverse Problems in Imaging.pdf:/home/javisty/Zotero/storage/65IFF7CB/2005.06001.pdf:application/pdf},
}

@article{abdolali_simplex-structured_2021,
	title = {Simplex-{Structured} {Matrix} {Factorization}: {Sparsity}-based {Identifiability} and {Provably} {Correct} {Algorithms}},
	volume = {3},
	issn = {2577-0187},
	shorttitle = {Simplex-{Structured} {Matrix} {Factorization}},
	url = {http://arxiv.org/abs/2007.11446},
	doi = {10.1137/20M1354982},
	abstract = {In this paper, we provide novel algorithms with identiﬁability guarantees for simplex-structured matrix factorization (SSMF), a generalization of nonnegative matrix factorization. Current stateof-the-art algorithms that provide identiﬁability results for SSMF rely on the suﬃciently scattered condition (SSC) which requires the data points to be well spread within the convex hull of the basis vectors. The conditions under which our proposed algorithms recover the unique decomposition is in most cases much weaker than the SSC. We only require to have d points on each facet of the convex hull of the basis vectors whose dimension is d − 1. The key idea is based on extracting facets containing the largest number of points. We illustrate the eﬀectiveness of our approach on synthetic data sets and hyperspectral images, showing that it outperforms state-of-the-art SSMF algorithms as it is able to handle higher noise levels, rank deﬁcient matrices, outliers, and input data that highly violates the SSC.},
	language = {en},
	number = {2},
	urldate = {2021-10-21},
	journal = {SIAM Journal on Mathematics of Data Science},
	author = {Abdolali, Maryam and Gillis, Nicolas},
	month = jan,
	year = {2021},
	note = {arXiv: 2007.11446},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
	pages = {593--623},
	annote = {Comment: 38 pages},
	file = {Abdolali_Gillis_2021_Simplex-Structured Matrix Factorization.pdf:/home/javisty/Zotero/storage/6EGVMH6K/Abdolali_Gillis_2021_Simplex-Structured Matrix Factorization.pdf:application/pdf},
}

@article{bolte_proximal_2014,
	title = {Proximal alternating linearized minimization for nonconvex and nonsmooth problems},
	volume = {146},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/10.1007/s10107-013-0701-9},
	doi = {10.1007/s10107-013-0701-9},
	abstract = {We introduce a proximal alternating linearized minimization (PALM) algorithm for solving a broad class of nonconvex and nonsmooth minimization problems. Building on the powerful Kurdyka–Łojasiewicz property, we derive a self-contained convergence analysis framework and establish that each bounded sequence generated by PALM globally converges to a critical point. Our approach allows to analyze various classes of nonconvex-nonsmooth problems and related nonconvex proximal forward–backward algorithms with semi-algebraic problem’s data, the later property being shared by many functions arising in a wide variety of fundamental applications. A by-product of our framework also shows that our results are new even in the convex setting. As an illustration of the results, we derive a new and simple globally convergent algorithm for solving the sparse nonnegative matrix factorization problem.},
	language = {en},
	number = {1-2},
	urldate = {2019-10-28},
	journal = {Mathematical Programming},
	author = {Bolte, Jérôme and Sabach, Shoham and Teboulle, Marc},
	month = aug,
	year = {2014},
	note = {tex.ids= Bolte2014b},
	pages = {459--494},
	file = {Bolte et al. - 2014 - Proximal alternating linearized minimization for n.pdf:/home/javisty/Zotero/storage/HJCKM9UI/palm.pdf:application/pdf},
}

@article{condat_primaldual_2013,
	title = {A {Primal}–{Dual} {Splitting} {Method} for {Convex} {Optimization} {Involving} {Lipschitzian}, {Proximable} and {Linear} {Composite} {Terms}},
	volume = {158},
	issn = {0022-3239, 1573-2878},
	url = {http://link.springer.com/10.1007/s10957-012-0245-9},
	doi = {10.1007/s10957-012-0245-9},
	abstract = {We propose a new ﬁrst-order splitting algorithm for solving jointly the primal and dual formulations of large-scale convex minimization problems involving the sum of a smooth function with Lipschitzian gradient, a nonsmooth proximable function, and linear composite functions. This is a full splitting approach, in the sense that the gradient and the linear operators involved are applied explicitly without any inversion, while the nonsmooth functions are processed individually via their proximity operators. This work brings together and notably extends several classical splitting schemes, like the forward–backward and Douglas–Rachford methods, as well as the recent primal–dual method of Chambolle and Pock designed for problems with linear composite terms.},
	language = {en},
	number = {2},
	urldate = {2019-10-28},
	journal = {Journal of Optimization Theory and Applications},
	author = {Condat, Laurent},
	month = aug,
	year = {2013},
	pages = {460--479},
	file = {Full Text:/home/javisty/Zotero/storage/75C8RTD4/Condat - 2013 - A Primal–Dual Splitting Method for Convex Optimiza.pdf:application/pdf},
}

@article{condat_fast_2016,
	title = {Fast projection onto the simplex and the \$\${\textbackslash}pmb \{l\}\_{\textbackslash}mathbf \{1\}\$\$ l 1 ball},
	volume = {158},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/10.1007/s10107-015-0946-6},
	doi = {10.1007/s10107-015-0946-6},
	abstract = {A new algorithm is proposed to project, exactly and in ﬁnite time, a vector of arbitrary size onto a simplex or an ℓ1-norm ball. It can be viewed as a GaussSeidel-like variant of Michelot’s variable ﬁxing algorithm; that is, the threshold used to ﬁx the variables is updated after each element is read, instead of waiting for a full reading pass over the list of non-ﬁxed elements. This algorithm is empirically demonstrated to be faster than existing methods.},
	language = {en},
	number = {1-2},
	urldate = {2021-11-25},
	journal = {Mathematical Programming},
	author = {Condat, Laurent},
	month = jul,
	year = {2016},
	pages = {575--585},
	file = {Condat - 2016 - Fast projection onto the simplex and the \$\$pmb l.pdf:/home/javisty/Zotero/storage/CFEGSY65/simplexproj.pdf:application/pdf},
}

@article{chen_computing_2016,
	title = {Computing the proximity operator of the ℓp norm with 0 {\textless} p {\textless} 1},
	volume = {10},
	issn = {1751-9683},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/iet-spr.2015.0244},
	doi = {10.1049/iet-spr.2015.0244},
	abstract = {Sparse modelling with the ℓp norm of 0 ≤ p ≤ 1 requires the availability of the proximity operator of the ℓp norm. The proximity operators of the ℓ0 and ℓ1 norms are the well-known hard- and soft-thresholding estimators, respectively. In this study, the authors give a complete study on the properties of the proximity operator of the ℓp norm. Based on these properties, explicit formulas of the proximity operators of the ℓ1/2 norm and ℓ2/3 norm are derived with simple proofs; for other values of p, an iterative Newton's method is developed to compute the proximity operator of the ℓp norm by fully exploring the available proximity operators of the ℓ0, ℓ1/2, ℓ2/3, and ℓ1 norms. As applications, the proximity operator of the ℓp norm with 0 ≤ p ≤ 1 is applied to the ℓp -regularisation for compressive sensing and image restoration.},
	language = {en},
	number = {5},
	urldate = {2021-12-09},
	journal = {IET Signal Processing},
	author = {Chen, Feishe and Shen, Lixin and Suter, Bruce W.},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1049/iet-spr.2015.0244},
	keywords = {image restoration, compressive sensing, hard-thresholding estimators, image coding, iterative Newton method, ℓp -regularisation, Newton method, proximity operator, soft-thresholding estimators, sparse modelling},
	pages = {557--565},
	file = {Full Text PDF:/home/javisty/Zotero/storage/SPXFEKYG/Chen et al. - 2016 - Computing the proximity operator of the ℓp norm wi.pdf:application/pdf;Snapshot:/home/javisty/Zotero/storage/FLMXL2SS/iet-spr.2015.html:text/html},
}

@article{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2021-12-09},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980
version: 5},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv Fulltext PDF:/home/javisty/Zotero/storage/R3XED4AR/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/home/javisty/Zotero/storage/5EVLSSZB/1412.html:text/html},
}

@article{strongin_global_2020,
	title = {Global optimization method with dual {Lipschitz} constant estimates for problems with non-convex constraints},
	volume = {24},
	issn = {1433-7479},
	url = {https://doi.org/10.1007/s00500-020-05078-1},
	doi = {10.1007/s00500-020-05078-1},
	abstract = {This paper considers the constrained global optimization problems, in which the functions are of the “black-box” type and satisfy the Lipschitz condition. The algorithms for solving the problems of this class require the use of adequate estimates of the a priori unknown Lipschitz constants for the problem functions. A novel approach presented in this paper is based on a simultaneous use of two estimates of the Lipschitz constant: an overestimated and an underestimated one. The upper estimate provides the global convergence, whereas the lower one reduces the number of trials necessary to find the global optimizer with the required accuracy. The considered algorithm for solving the constrained problems does not use the ideas of the penalty function method; each constraint of the problem is accounted for separately. The convergence conditions of the proposed algorithm are formulated in the corresponding theorem. The results of the numerical experiments on a series of multiextremal problems with non-convex constraints demonstrating the efficiency of the proposed scheme of dual Lipschitz constant estimates are presented.},
	language = {en},
	number = {16},
	urldate = {2021-12-09},
	journal = {Soft Computing},
	author = {Strongin, Roman and Barkalov, Konstantin and Bevzuk, Semen},
	month = aug,
	year = {2020},
	pages = {11853--11865},
	file = {Strongin et al. - 2020 - Global optimization method with dual Lipschitz con.pdf:/home/javisty/Zotero/storage/HVDLURGU/Strongin et al. - 2020 - Global optimization method with dual Lipschitz con.pdf:application/pdf},
}

@article{duchi_adaptive_nodate,
	title = {Adaptive {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization}},
	abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to ﬁnd needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which signiﬁcantly simpliﬁes setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efﬁcient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	language = {en},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	pages = {39},
	file = {Duchi et al. - Adaptive Subgradient Methods for Online Learning a.pdf:/home/javisty/Zotero/storage/LJDPIIIG/Duchi et al. - Adaptive Subgradient Methods for Online Learning a.pdf:application/pdf},
}

@article{condat_generic_2014,
	title = {A {Generic} {Proximal} {Algorithm} for {Convex} {Optimization}—{Application} to {Total} {Variation} {Minimization}},
	volume = {21},
	issn = {1558-2361},
	doi = {10.1109/LSP.2014.2322123},
	abstract = {We propose new optimization algorithms to minimize a sum of convex functions, which may be smooth or not and composed or not with linear operators. This generic formulation encompasses various forms of regularized inverse problems in imaging. The proposed algorithms proceed by splitting: the gradient or proximal operators of the functions are called individually, without inner loop or linear system to solve at each iteration. The algorithms are easy to implement and have proven convergence to an exact solution. The classical Douglas-Rachford and forward-backward splitting methods, as well as the recent and efficient algorithm of Chambolle-Pock, are recovered as particular cases. The application to inverse imaging problems regularized by the total variation is detailed.},
	number = {8},
	journal = {IEEE Signal Processing Letters},
	author = {Condat, Laurent},
	month = aug,
	year = {2014},
	note = {tex.ids= Condat2014
conferenceName: IEEE Signal Processing Letters},
	keywords = {Inverse problems, Optimization, Signal processing algorithms, Convergence, Convex functions, Convex nonsmooth optimization, Hilbert space, Imaging, proximal splitting algorithm, regularized inverse problem, total variation},
	pages = {985--989},
	file = {IEEE Xplore Abstract Record:/home/javisty/Zotero/storage/PRMMRR29/6810809.html:text/html;Condat_2014_A Generic Proximal Algorithm for Convex Optimization—Application to Total.pdf:/home/javisty/Zotero/storage/4IXIYPD3/Condat-optim-SPL-2014.pdf:application/pdf;Condat-optim-SPL-2014.pdf:/home/javisty/Zotero/storage/U3Q5PH2B/Condat-optim-SPL-2014.pdf:application/pdf},
}

@article{chen_convergence_2019,
	title = {On the {Convergence} of {A} {Class} of {Adam}-{Type} {Algorithms} for {Non}-{Convex} {Optimization}},
	url = {http://arxiv.org/abs/1808.02941},
	abstract = {This paper studies a class of adaptive gradient based momentum algorithms that update the search directions and learning rates simultaneously using past gradients. This class, which we refer to as the “Adam-type”, includes the popular algorithms such as Adam (Kingma \& Ba, 2014) , AMSGrad (Reddi et al., 2018) , AdaGrad (Duchi et al., 2011). Despite their popularity in training deep neural networks (DNNs), the convergence of these algorithms for solving non-convex problems remains an open question.},
	language = {en},
	urldate = {2021-12-09},
	journal = {arXiv:1808.02941 [cs, math, stat]},
	author = {Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
	month = mar,
	year = {2019},
	note = {tex.ids= Chen2019b
arXiv: 1808.02941},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {Chen et al. - 2019 - On the Convergence of A Class of Adam-Type Algorit.pdf:/home/javisty/Zotero/storage/5QLUKEVJ/1808.02941.pdf:application/pdf;Chen et al. - 2019 - On the Convergence of A Class of Adam-Type Algorit.pdf:/home/javisty/Zotero/storage/C23A4TC8/Chen et al. - 2019 - On the Convergence of A Class of Adam-Type Algorit.pdf:application/pdf},
}

@article{adly_decomposition_nodate,
	title = {On a decomposition formula for the proximal operator of the sum of two convex functions},
	abstract = {The main result of the present theoretical paper is an original decomposition formula for the proximal operator of the sum of two proper, lower semicontinuous and convex functions f and g. For this purpose, we introduce a new operator, called f -proximal operator of g and denoted by proxfg , that generalizes the classical notion. Then we prove the decomposition formula proxf+g = proxf ◦ proxfg . After collecting several properties and characterizations of proxfg , we prove that it coincides with the ﬁxed points of a generalized version of the classical Douglas-Rachford operator. This relationship is used for the construction of a weakly convergent algorithm that computes numerically this new operator proxfg , and thus, from the decomposition formula, allows to compute numerically proxf+g. It turns out that this algorithm was already considered and implemented in previous works, showing that proxfg is already present (in a hidden form) and useful for numerical purposes in the existing literature. However, to the best of our knowledge, it has never been explicitly expressed in a closed formula and neither been deeply studied from a theoretical point of view. The present paper contributes to ﬁll this gap in the literature. Finally we give an illustration of the usefulness of the decomposition formula in the context of sensitivity analysis of linear variational inequalities of second kind in a Hilbert space.},
	language = {en},
	author = {Adly, Samir and Bourdin, Loïc and Caubet, Fabien},
	pages = {21},
	file = {Adly et al. - On a decomposition formula for the proximal operat.pdf:/home/javisty/Zotero/storage/5XXBE4ZZ/Adly et al. - On a decomposition formula for the proximal operat.pdf:application/pdf},
}

@inproceedings{yu_decomposing_2013,
	title = {On {Decomposing} the {Proximal} {Map}},
	volume = {26},
	url = {https://papers.nips.cc/paper/2013/hash/98dce83da57b0395e163467c9dae521b-Abstract.html},
	urldate = {2021-12-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yu, Yao-Liang},
	year = {2013},
	file = {Yu_2013_On Decomposing the Proximal Map.pdf:/home/javisty/Zotero/storage/GJFXXUXX/NIPS-2013-on-decomposing-the-proximal-map-Paper.pdf:application/pdf},
}

@article{cherni_spoq_2020,
	title = {{SPOQ} \${\textbackslash}ell \_p\$-{Over}-\${\textbackslash}ell \_q\$ {Regularization} for {Sparse} {Signal} {Recovery} {Applied} to {Mass} {Spectrometry}},
	volume = {68},
	issn = {1941-0476},
	doi = {10.1109/TSP.2020.3025731},
	abstract = {Underdetermined or ill-posed inverse problems require additional information for sound solutions with tractable optimization algorithms. Sparsity yields consequent heuristics to that matter, with numerous applications in signal restoration, image recovery, or machine learning. Since the ℓ0 count measure is barely tractable, many statistical or learning approaches have invested in computable proxies, such as the ℓ1 norm. However, the latter does not exhibit the desirable property of scale invariance for sparse data. Extending the SOOT Euclidean/Taxicab ℓ1-over-ℓ2 norm-ratio initially introduced for blind deconvolution, we propose SPOQ, a family of smoothed (approximately) scale-invariant penalty functions. It consists of a Lipschitz-differentiable surrogate for lP-over-lQ quasi-norm/norm ratios with p ∈ ]0, 2[ and q ≥ 2. This surrogate is embedded into a novel majorize-minimize trust-region approach, generalizing the variable metric forward-backward algorithm. For naturally sparse mass-spectrometry signals, we show that SPOQ significantly outperforms ℓ0, ℓ1, Cauchy, Welsch, SCAD and CEL0 penalties on several performance measures. Guidelines on SPOQ hyperparameters tuning are also provided, suggesting simple data-driven choices.},
	journal = {IEEE Transactions on Signal Processing},
	author = {Cherni, Afef and Chouzenoux, Emilie and Duval, Laurent and Pesquet, Jean-Christophe},
	year = {2020},
	note = {tex.ids= Cherni2020
conferenceName: IEEE Transactions on Signal Processing},
	keywords = {Inverse problems, Optimization, Signal processing algorithms, Convergence, Image restoration, majorize-minimize method, mass spectrometry, Mass spectroscopy, Minimization, nonconvex optimization, nonsmooth optimization, norm ratio, quasinorm, sparsity},
	pages = {6070--6084},
	file = {IEEE Xplore Abstract Record:/home/javisty/Zotero/storage/RNTSAWJP/9201484.html:text/html;Cherni et al_2020_SPOQ \$-ell _p\$-Over-\$-ell _q\$ Regularization for Sparse Signal Recovery Applied.pdf:/home/javisty/Zotero/storage/N4TTTJY7/Cherni et al_2020_SPOQ \$-ell _p\$-Over-\$-ell _q\$ Regularization for Sparse Signal Recovery Applied.pdf:application/pdf;Cherni et al. - 2020 - SPOQ \$ell _p\$-Over-\$ell _q\$ Regularization for S.pdf:/home/javisty/Zotero/storage/4MLMJCME/Cherni_A_2020_PREPRINT_spoq_lpolqrssrams.pdf:application/pdf},
}

@article{de_convergence_2018,
	title = {Convergence guarantees for {RMSProp} and {ADAM} in non-convex optimization and an empirical comparison to {Nesterov} acceleration},
	url = {http://arxiv.org/abs/1807.06766},
	abstract = {RMSProp and ADAM continue to be extremely popular algorithms for training neural nets but their theoretical convergence properties have remained unclear. Further, recent work has seemed to suggest that these algorithms have worse generalization properties when compared to carefully tuned stochastic gradient descent or its momentum variants. In this work, we make progress towards a deeper understanding of ADAM and RMSProp in two ways. First, we provide proofs that these adaptive gradient algorithms are guaranteed to reach criticality for smooth non-convex objectives, and we give bounds on the running time. Next we design experiments to empirically study the convergence and generalization properties of RMSProp and ADAM against Nesterov's Accelerated Gradient method on a variety of common autoencoder setups and on VGG-9 with CIFAR-10. Through these experiments we demonstrate the interesting sensitivity that ADAM has to its momentum parameter \${\textbackslash}beta\_1\$. We show that at very high values of the momentum parameter (\${\textbackslash}beta\_1 = 0.99\$) ADAM outperforms a carefully tuned NAG on most of our experiments, in terms of getting lower training and test losses. On the other hand, NAG can sometimes do better when ADAM's \${\textbackslash}beta\_1\$ is set to the most commonly used value: \${\textbackslash}beta\_1 = 0.9\$, indicating the importance of tuning the hyperparameters of ADAM to get better generalization performance. We also report experiments on different autoencoders to demonstrate that NAG has better abilities in terms of reducing the gradient norms, and it also produces iterates which exhibit an increasing trend for the minimum eigenvalue of the Hessian of the loss function at the iterates.},
	urldate = {2022-01-07},
	journal = {arXiv:1807.06766 [cs, math, stat]},
	author = {De, Soham and Mukherjee, Anirbit and Ullah, Enayat},
	month = nov,
	year = {2018},
	note = {arXiv: 1807.06766},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	annote = {Comment: Presented on 14th July 2018 at the ICML Workshop on Modern Trends in Nonconvex Optimization for Machine Learning. In this version, we have made changes to the setup of our Theorem 3.1, and added additional experimental results},
	file = {arXiv.org Snapshot:/home/javisty/Zotero/storage/I84A6T7K/1807.html:text/html;De et al_2018_Convergence guarantees for RMSProp and ADAM in non-convex optimization and an.pdf:/home/javisty/Zotero/storage/J48CALUW/De et al_2018_Convergence guarantees for RMSProp and ADAM in non-convex optimization and an.pdf:application/pdf},
}

@article{kervadec_constrained_2020,
	title = {Constrained {Deep} {Networks}: {Lagrangian} {Optimization} via {Log}-{Barrier} {Extensions}},
	shorttitle = {Constrained {Deep} {Networks}},
	url = {http://arxiv.org/abs/1904.04205},
	abstract = {This study investigates the optimization aspects of imposing hard inequality constraints on the outputs of CNNs. In the context of deep networks, constraints are commonly handled with penalties for their simplicity, and despite their well-known limitations. Lagrangian-dual optimization has been largely avoided, except for a few recent works, mainly due to the computational complexity and stability/convergence issues caused by alternating explicit dual updates/projections and stochastic optimization. Several studies showed that, surprisingly for deep CNNs, the theoretical and practical advantages of Lagrangian optimization over penalties do not materialize in practice. We propose log-barrier extensions, which approximate Lagrangian optimization of constrained-CNN problems with a sequence of unconstrained losses. Unlike standard interior-point and log-barrier methods, our formulation does not need an initial feasible solution. Furthermore, we provide a new technical result, which shows that the proposed extensions yield an upper bound on the duality gap. This generalizes the dualitygap result of standard log-barriers, yielding sub-optimality certiﬁcates for feasible solutions. While sub-optimality is not guaranteed for non-convex problems, our result shows that log-barrier extensions are a principled way to approximate Lagrangian optimization for constrained CNNs via implicit dual variables. We report comprehensive weakly supervised segmentation experiments, with various constraints, showing that our formulation outperforms substantially the existing constrained-CNN methods, both in terms of accuracy, constraint satisfaction and training stability, more so when dealing with a large number of constraints.},
	language = {en},
	urldate = {2022-01-07},
	journal = {arXiv:1904.04205 [cs]},
	author = {Kervadec, Hoel and Dolz, Jose and Yuan, Jing and Desrosiers, Christian and Granger, Eric and Ayed, Ismail Ben},
	month = apr,
	year = {2020},
	note = {arXiv: 1904.04205},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Kervadec et al. - 2020 - Constrained Deep Networks Lagrangian Optimization.pdf:/home/javisty/Zotero/storage/FKPT5ZFQ/kervadec2020.pdf:application/pdf},
}

@book{jin_data-driven_2021,
	address = {Cham},
	series = {Studies in {Computational} {Intelligence}},
	title = {Data-{Driven} {Evolutionary} {Optimization}: {Integrating} {Evolutionary} {Computation}, {Machine} {Learning} and {Data} {Science}},
	volume = {975},
	isbn = {978-3-030-74639-1 978-3-030-74640-7},
	shorttitle = {Data-{Driven} {Evolutionary} {Optimization}},
	url = {https://link.springer.com/10.1007/978-3-030-74640-7},
	language = {en},
	urldate = {2022-01-13},
	publisher = {Springer International Publishing},
	author = {Jin, Yaochu and Wang, Handing and Sun, Chaoli},
	year = {2021},
	doi = {10.1007/978-3-030-74640-7},
	file = {Jin et al. - 2021 - Data-Driven Evolutionary Optimization Integrating.pdf:/home/javisty/Zotero/storage/67HC58W6/Jin et al. - 2021 - Data-Driven Evolutionary Optimization Integrating.pdf:application/pdf},
}

@article{yang_constrained_2014,
	title = {Constrained {Nonconvex} {Nonsmooth} {Optimization} via {Proximal} {Bundle} {Method}},
	volume = {163},
	issn = {1573-2878},
	url = {https://doi.org/10.1007/s10957-014-0523-9},
	doi = {10.1007/s10957-014-0523-9},
	abstract = {In this paper, we consider a constrained nonconvex nonsmooth optimization, in which both objective and constraint functions may not be convex or smooth. With the help of the penalty function, we transform the problem into an unconstrained one and design an algorithm in proximal bundle method in which local convexification of the penalty function is utilized to deal with it. We show that, if adding a special constraint qualification, the penalty function can be an exact one, and the sequence generated by our algorithm converges to the KKT points of the problem under a moderate assumption. Finally, some illustrative examples are given to show the good performance of our algorithm.},
	language = {en},
	number = {3},
	urldate = {2022-01-13},
	journal = {Journal of Optimization Theory and Applications},
	author = {Yang, Yang and Pang, Liping and Ma, Xuefei and Shen, Jie},
	month = dec,
	year = {2014},
	pages = {900--925},
	file = {Springer Full Text PDF:/home/javisty/Zotero/storage/I35C964I/Yang et al. - 2014 - Constrained Nonconvex Nonsmooth Optimization via P.pdf:application/pdf},
}

@article{le_bundle_nodate,
	title = {Bundle {Methods} for {Machine} {Learning}},
	abstract = {We present a globally convergent method for regularized risk minimization problems. Our method applies to Support Vector estimation, regression, Gaussian Processes, and any other regularized risk minimization setting which leads to a convex optimization problem. SVMPerf can be shown to be a special case of our approach. In addition to the uniﬁed framework we present tight convergence bounds, which show that our algorithm converges in O(1/ ) steps to precision for general convex problems and in O(log(1/ )) steps for continuously differentiable problems. We demonstrate in experiments the performance of our approach.},
	language = {en},
	author = {Le, Quoc V and Smola, Alex J},
	pages = {8},
	file = {Le and Smola - Bundle Methods for Machine Learning.pdf:/home/javisty/Zotero/storage/3TFUPFI3/Le and Smola - Bundle Methods for Machine Learning.pdf:application/pdf},
}
